# -*- coding: utf-8 -*-
# tts_module.py
# TTS ë©”ì¸ ì¸í„°í˜ì´ìŠ¤ (API í˜¸ì¶œ ë° ì›Œí¬í”Œë¡œìš° ê´€ë¦¬)

import re
import os
import time
import shutil
import requests
import hashlib
import asyncio
from typing import List, Dict, Optional
from pathlib import Path
from dotenv import load_dotenv

# ê³µí†µ ë¡œì§ ëª¨ë“ˆ ì„í¬íŠ¸
import tts_core
from tts_core import add_audio_to_video, concat_videos_with_audio, get_audio_duration

# í´ë¡œë°” API ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ìš°ì„ , í´ë°±ìœ¼ë¡œ ê¸°ë³¸ê°’)
import os
from dotenv import load_dotenv
load_dotenv()

# ==========================================
# 1. API ì„¤ì • (Clova / OpenAI / GEMINI ë“±)
# ==========================================
CLOVA_CLIENT_ID = os.getenv("CLOVA_CLIENT_ID", "ozpoytlz95")
CLOVA_CLIENT_SECRET = os.getenv("CLOVA_CLIENT_SECRET", "34c2g67KpznehFvEOzamxoqrrfSsQP5tzey1dwi2")
CLOVA_ENDPOINT = "https://naveropenapi.apigw.ntruss.com/tts-premium/v1/tts"

# [OpenAI ì„¤ì •]
try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    print("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. (pip install openai)")

OPENAI_TTS_MODEL = "gpt-4o-mini-tts"

from google.cloud import texttospeech
from google.oauth2 import service_account

# ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ì´ë¦„ (tts_module.pyì™€ ê°™ì€ í´ë”)
SERVICE_ACCOUNT_FILE = "tts-gemini-env.json"

import wave  # WAV íŒŒì¼ ì €ì¥ìš©
# Google GenAI SDK ì„¤ì • preview ë²„ì „ ì‚¬ìš©
try:
    from google import genai
    from google.genai import types
    HAS_GOOGLE_GENAI = True
except ImportError:
    HAS_GOOGLE_GENAI = False
    print("âš ï¸ google-genai ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. (pip install google-genai)")

# [í•µì‹¬] Gemini ëª¨ë¸ ë§¤í•‘ (ì—”ì§„ í‚¤ -> ì‹¤ì œ ëª¨ë¸ ID)
GEMINI_MODELS = {
    "default": "gemini-2.5-flash-tts",  # ê¸°ë³¸ê°’
    "flash": "gemini-2.5-flash-tts",    # engine="gemini-flash"
    "pro": "gemini-2.5-pro-tts",        # engine="gemini-pro"
}

# [ì—”ì§„ë³„ ê¸€ììˆ˜ ì œí•œ]
LIMITS = {
    "clova": 2000,
    "gpt": 2000,
    "gemini": 2000
}

# [GPT í™”ì ë§¤í•‘] (#ìë™ ë°°ì • ì¶”í›„ ìƒì„¸ ìˆ˜ì • í•„ìš”í•¨)
GPT_VOICE_MAP = {
    "narrator": "marin",       # ë‚˜ë ˆì´ì…˜ ìµœì 
    "narrator_male": "cedar",
    "young_female": "coral",   # ë°ì€ í†¤
    "adult_female": "sage",    # ì°¨ë¶„í•œ í†¤
    "child_female": "ballad",  # ê°ì„±ì  (ì•„ì´ ëŒ€ìš©)
    "child_male": "ash",       # (ëŒ€ì•ˆ)
    "young_male": "verse",     # ë“œë¼ë§ˆí‹±
    "adult_male": "ash",       # ê°•í•œ í†¤
    "elder_female": "shimmer",
    "elder_male": "onyx",      # ì €ìŒ
    "default": "alloy"
}

#  Gemini í™”ì ë§¤í•‘ (ê°€ì´ë“œ ê¸°ë°˜ ë³„ìë¦¬ ì´ë¦„)
GEMINI_VOICE_MAP = {
    "narrator": "Puck",       # ë‚¨ì„± (ë‚´ë ˆì´ì…˜ìš©)
    "narrator_male": "Puck",
    "young_female": "Aoede",  # ì—¬ì„± (ë°ìŒ)
    "adult_female": "Kore",   # ì—¬ì„± (ì°¨ë¶„í•¨)
    "child_female": "Aoede",  # (ëŒ€ì•ˆ)
    "child_male": "Puck",     # (ëŒ€ì•ˆ)
    "young_male": "Fenrir",   # ë‚¨ì„± (í™œê¸°ì°¸)
    "adult_male": "Charon",   # ë‚¨ì„± (êµµìŒ)
    "elder_female": "Leda",   # ì—¬ì„±
    "elder_male": "Charon",   # ë‚¨ì„±
    "default": "Puck"
}

# Edge TTS í´ë°± ì§€ì› (ë¬´ë£Œ, ë‹¤ì¤‘ ëª©ì†Œë¦¬)
try:
    import edge_tts
    import asyncio
    HAS_EDGE_TTS = True
except ImportError:
    HAS_EDGE_TTS = False


# Edge TTS í•œêµ­ì–´ ëª©ì†Œë¦¬ ë§¤í•‘ (Clova ì‹¤íŒ¨ ì‹œ í´ë°±ìš©)
EDGE_TTS_VOICES = {
    'child_male': 'ko-KR-InJoonNeural',
    'child_female': 'ko-KR-SunHiNeural',
    'young_male': 'ko-KR-InJoonNeural',
    'young_female': 'ko-KR-SunHiNeural',
    'adult_male': 'ko-KR-BongJinNeural',
    'adult_female': 'ko-KR-YuJinNeural',
    'elder_male': 'ko-KR-GookMinNeural',
    'elder_female': 'ko-KR-SoonBokNeural',
    'narrator': 'ko-KR-SunHiNeural',
    'default': 'ko-KR-SunHiNeural',
}

# ê°ì • ì§€ì› í™”ì (ê³µì‹ ë¬¸ì„œ ê¸°ì¤€)
EMOTION_SUPPORTED = {
    "nara": {"anger_supported": False},
    "vara": {"anger_supported": True},
    "vmikyung": {"anger_supported": True},
    "vdain": {"anger_supported": True},
    "vyuna": {"anger_supported": True},
    "vgoeun": {"anger_supported": True},
    "vdaeseong": {"anger_supported": True},
}

# PRO í™”ìì˜ emotion-strength ì§€ì›
EMOTION_STRENGTH_SUPPORTED = {"vara", "vmikyung", "vdain", "vyuna"}


# ==========================================
# 2. ë‚´ë¶€ API í˜¸ì¶œ í•¨ìˆ˜ (Drivers)
# ==========================================


def get_edge_voice_type(speaker: str) -> str:
    """Clova í™”ì â†’ Edge TTS ëª©ì†Œë¦¬ íƒ€ì… ë§¤í•‘"""
    speaker_lower = speaker.lower()

    # 1. Clova í™”ì ID ì§ì ‘ ë§¤í•‘ (VOICE_ALIASES ì—­ë§¤í•‘)
    CLOVA_TO_EDGE = {
        # ì•„ë™
        'nhajun': 'child_male', 'ndain': 'child_female', 'nmammon': 'child_female',
        # ì²­ë…„ ì—¬ì„±
        'nara': 'young_female', 'nara_call': 'young_female', 'vyuna': 'young_female',
        'vara': 'young_female', 'vmikyung': 'young_female', 'vdain': 'young_female',
        'vgoeun': 'young_female', 'nsujin': 'young_female', 'nsinu': 'young_female',
        # ì²­ë…„ ë‚¨ì„±
        'nwoof': 'young_male', 'noyj': 'young_male', 'nyejun': 'young_male',
        'njooahn': 'young_male', 'vdaeseong': 'adult_male',
        # ì„±ì¸ ì—¬ì„±
        'nyejin': 'adult_female', 'nmiyeon': 'adult_female', 'nheeyeon': 'adult_female',
        'ngaram': 'adult_female',
        # ì„±ì¸ ë‚¨ì„±
        'nminsang': 'adult_male', 'nminho': 'elder_male', 'nwontak': 'adult_male',
        'nkwangsu': 'adult_male', 'njonghyun': 'adult_male', 'njoonyoung': 'adult_male',
        # ë‚˜ë ˆì´í„°
        'njiyun': 'narrator',
        # íŠ¹ìˆ˜
        'nmeow': 'child_female',
    }

    # Clova í™”ì ID ì§ì ‘ ë§¤ì¹­
    if speaker_lower in CLOVA_TO_EDGE:
        return CLOVA_TO_EDGE[speaker_lower]

    # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤í•‘ (ê¸°ì¡´ ë¡œì§)
    if any(k in speaker_lower for k in ['child', 'ì•„ì´', 'ê¼¬ë§ˆ', 'ì–´ë¦°']):
        if any(k in speaker_lower for k in ['female', 'ì—¬', 'ì†Œë…€']):
            return 'child_female'
        return 'child_male'
    elif any(k in speaker_lower for k in ['elder', 'í• ë¨¸ë‹ˆ', 'í• ì•„ë²„ì§€', 'ë…¸ì¸']):
        if any(k in speaker_lower for k in ['female', 'ì—¬', 'í• ë¨¸ë‹ˆ']):
            return 'elder_female'
        return 'elder_male'
    elif any(k in speaker_lower for k in ['adult', 'ì–´ë¥¸', 'ì•„ì €ì”¨', 'ì•„ì¤Œë§ˆ']):
        if any(k in speaker_lower for k in ['female', 'ì—¬', 'ì•„ì¤Œë§ˆ', 'ì—„ë§ˆ']):
            return 'adult_female'
        return 'adult_male'
    elif any(k in speaker_lower for k in ['narrator', 'ë‚˜ë ˆì´í„°']):
        return 'narrator'
    elif any(k in speaker_lower for k in ['female', 'ì—¬']):
        return 'young_female'
    elif any(k in speaker_lower for k in ['male', 'ë‚¨']):
        return 'young_male'

    return 'default'

def edge_tts_fallback(text: str, output_path: str, voice_type: str = 'default') -> bool:
    """
    Edge TTS í´ë°± (Clova ì‹¤íŒ¨ ì‹œ) - ë¬´ë£Œ, ë‹¤ì¤‘ ëª©ì†Œë¦¬

    Args:
        text: ë³€í™˜í•  í…ìŠ¤íŠ¸
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        voice_type: ëª©ì†Œë¦¬ íƒ€ì… (EDGE_TTS_VOICES í‚¤)

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    if not HAS_EDGE_TTS:
        print("    [WARN] Edge TTS not installed (pip install edge-tts)")
        return False

    try:
        edge_voice = EDGE_TTS_VOICES.get(voice_type, EDGE_TTS_VOICES['default'])

        async def _generate():
            communicate = edge_tts.Communicate(text, edge_voice)
            await communicate.save(output_path)

        asyncio.run(_generate())
        print(f"    [OK] Edge TTS fallback: {output_path} ({edge_voice})")
        return True
    except Exception as e:
        print(f"    [ERR] Edge TTS failed: {e}")
        return False


# ==========================================
# Gemini ìƒì„± í•¨ìˆ˜ 
# ==========================================
def _generate_with_gemini(
    text: str, 
    output_path: str, 
    speaker: str, 
    speed: int, 
    style_prompt: str, 
    model_name: str
) -> bool:
    """
    Google Cloud Text-to-Speech (Service Account) ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ìƒì„±
    """
    # 1. ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
    try:
        current_dir = Path(__file__).parent
        key_path = current_dir / SERVICE_ACCOUNT_FILE
        
        if not key_path.exists():
            print(f"    [ERR] í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {key_path}")
            return False

        cred = service_account.Credentials.from_service_account_file(key_path)
        client = texttospeech.TextToSpeechClient(credentials=cred)

    except Exception as e:
        print(f"    [ERR] Google Client ì¸ì¦ ì‹¤íŒ¨: {e}")
        return False

    # ì¬ì‹œë„ ë¡œì§ ì„¤ì •
    max_retries = 3
    base_wait_time = 2

    for attempt in range(max_retries):
        try:
            # 2. í™”ì ë§¤í•‘
            voice_name = GEMINI_VOICE_MAP.get(speaker, "Puck")
            
            # -------------------------------------------------------------
            #  Flash ëª¨ë¸ì€ í”„ë¡¬í”„íŠ¸ ì™„ì „ ì œê±° í”„ë¡¬í”„íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì½ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒ
            # -------------------------------------------------------------
            
            final_input_text = text
            
            # ëª¨ë¸ëª…ì— 'flash'ê°€ ë“¤ì–´ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ë¥¼ ì•„ì˜ˆ ë¬´ì‹œí•©ë‹ˆë‹¤.
            if "flash" in model_name.lower():
                # Flash: ê·¸ëƒ¥ í…ìŠ¤íŠ¸ë§Œ ë„˜ê¹€ (ê°€ì¥ ì•ˆì „)
                final_input_text = text
            else:
                # Pro: í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ì ìš©
                if style_prompt:
                    # "Act out..." ê°™ì€ ëª…ë ¹ì¡° ì œê±°í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ê²°í•©
                    safe_prompt = style_prompt.replace("Act out this line", "").replace("Speak", "").strip()
                    # Pro ëª¨ë¸ìš© í¬ë§· (í—¤ë” ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì¤„ë°”ê¿ˆìœ¼ë¡œë§Œ êµ¬ë¶„)
                    final_input_text = f"{safe_prompt}\n\n{text}"
                else:
                    final_input_text = text

            # 3. ì…ë ¥ ê°ì²´ ìƒì„±
            synthesis_input = texttospeech.SynthesisInput(
                text=final_input_text
            )

            # 4. ëª©ì†Œë¦¬ ë° ëª¨ë¸ ì„¤ì •
            voice = texttospeech.VoiceSelectionParams(
                language_code="ko-KR",
                name=voice_name,
                model_name=model_name,
            )

            # 5. ì˜¤ë””ì˜¤ ì„¤ì • (ì†ë„ ì¡°ì ˆ)
            gemini_rate = 1.0 - (speed * 0.1)
            gemini_rate = max(0.25, min(4.0, gemini_rate))

            audio_config = texttospeech.AudioConfig(
                audio_encoding=texttospeech.AudioEncoding.MP3,
                speaking_rate=gemini_rate
            )

            # 6. í•©ì„± ìš”ì²­
            response = client.synthesize_speech(
                input=synthesis_input,
                voice=voice,
                audio_config=audio_config,
            )

            # 7. íŒŒì¼ ì €ì¥
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, "wb") as out:
                out.write(response.audio_content)
                
            return True

        except Exception as e:
            error_msg = str(e)
            if "429" in error_msg or "ResourceExhausted" in error_msg or "Quota" in error_msg:
                wait_time = base_wait_time * (1** attempt)
                print(f"     [429 Quota] {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„ ({attempt+1}/{max_retries})...")
                time.sleep(wait_time)
            else:
                print(f"     Gemini TTS Error ({model_name}): {e}")
                return False
    
    print(f"     ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ë¡œ ì‹¤íŒ¨")
    return False

# GPT ìƒì„± í•¨ìˆ˜
def _generate_with_gpt(text: str, output_path: str, speaker: str, speed: float, instructions: str) -> bool:
    if not HAS_OPENAI:
        print("    [ERR] OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜")
        return False
        
    # í™”ì ë§¤í•‘
    voice_id = GPT_VOICE_MAP.get(speaker, GPT_VOICE_MAP["default"])
    
    # ì†ë„ ë³€í™˜ (Clova -5~5 -> GPT 0.25~4.0)
    gpt_speed = 1.0 - (speed * 0.1)
    gpt_speed = max(0.5, min(2.0, gpt_speed))

    try:
        client = OpenAI()
        with client.audio.speech.with_streaming_response.create(
            model=OPENAI_TTS_MODEL,
            voice=voice_id,
            input=text,
            speed=gpt_speed,
            instructions=instructions # [í•µì‹¬] ê°ì • í”„ë¡¬í”„íŠ¸
        ) as response:
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            response.stream_to_file(output_path)
        return True
    except Exception as e:
        print(f"     GPT API Error: {e}")
        return False

# Clova ìƒì„± í•¨ìˆ˜ (ì´ë¦„ ë³€ê²½ ì—†ìŒ, ë‚´ë¶€ ë¡œì§ë§Œ ë¶„ë¦¬ ê°€ëŠ¥í•˜ë‚˜ ê·¸ëŒ€ë¡œ ìœ ì§€)
def _generate_with_clova(text, output_path, speaker, speed, pitch, volume, emotion, emotion_strength):
    headers = {
        "X-NCP-APIGW-API-KEY-ID": CLOVA_CLIENT_ID,
        "X-NCP-APIGW-API-KEY": CLOVA_CLIENT_SECRET,
        "Content-Type": "application/x-www-form-urlencoded"
    }
    payload = {
        "speaker": speaker, "text": text, "speed": str(speed),
        "pitch": str(pitch), "volume": str(volume), "format": "mp3"
    }
    if emotion and speaker in EMOTION_SUPPORTED:
        if emotion == "angry" and not EMOTION_SUPPORTED[speaker].get("anger_supported"): emotion = "neutral"
        payload["emotion"] = emotion
        if emotion_strength: payload["emotion-strength"] = str(min(max(emotion_strength, 0), 2))

    for attempt in range(3):
        try:
            resp = requests.post(CLOVA_ENDPOINT, headers=headers, data=payload, timeout=30)
            if resp.status_code == 200:
                Path(output_path).parent.mkdir(parents=True, exist_ok=True)
                with open(output_path, "wb") as f: f.write(resp.content)
                return True
            elif resp.status_code >= 500: time.sleep(1)
            else: break
        except: time.sleep(1)
    return False

def _text_to_speech_single(
    text: str,
    output_path: str,
    speaker: str = "narrator",
    speed: int = 0,
    pitch: int = 0,
    volume: int = 0,
    emotion: Optional[str] = None,
    emotion_strength: Optional[int] = None,
    use_edge_fallback: bool = True,
    use_cache: bool = True,
    engine: str = "clova",      #  ì—”ì§„ ì„ íƒ
    style_prompt: str = ""      #  gpt, gemini ìš© í”„ë¡¬í”„íŠ¸
) -> bool:
    """
    ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ TTS ë³€í™˜ (ë‚´ë¶€ í•¨ìˆ˜)

    Args:
        emotion: ê°ì • (neutral, happy, sad, angry) - ì§€ì› í™”ìë§Œ
        emotion_strength: ê°ì • ê°•ë„ (0-2) - PRO í™”ìë§Œ
        use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
    """
    # 1. í™”ì ID ê²°ì • (Clovaì¼ ë•Œë§Œ ë³€í™˜)
    clova_speaker_id = speaker
    if engine == "clova":
        if not speaker.startswith(('n', 'v', 'd', 'm', 'c', 's')):
            clova_speaker_id = tts_core.VOICE_ALIASES.get(speaker, "njiyun")

    # 2. ìºì‹œ í‚¤ ìƒì„± (ì—”ì§„ê³¼ í”„ë¡¬í”„íŠ¸ í¬í•¨)
    cache_key = hashlib.md5(
        f"{text}|{speaker}|{engine}|{speed}|{pitch}|{style_prompt}".encode()
    ).hexdigest()

    if use_cache:
        cached_path = tts_core._TTS_CACHE.get(cache_key)
        if cached_path and Path(cached_path).exists():
            shutil.copy(cached_path, output_path)
            print(f"    [CACHE HIT] {output_path}")
            return True

    # 3. ì—”ì§„ë³„ í˜¸ì¶œ ë¶„ê¸°
    success = False
    engine_lower = engine.lower()
    if "gpt" in engine_lower:
        success = _generate_with_gpt(text, output_path, speaker, speed, style_prompt)
        
    elif "gemini" in engine_lower:
        # [í•µì‹¬] gemini-pro / gemini-flash êµ¬ë¶„ ë¡œì§
        if "pro" in engine_lower:
            target_model = GEMINI_MODELS["pro"]
        elif "flash" in engine_lower:
            target_model = GEMINI_MODELS["flash"]
        else:
            target_model = GEMINI_MODELS["default"] # ê·¸ëƒ¥ "gemini"ë¡œ ë“¤ì–´ì˜¨ ê²½ìš°
            
        success = _generate_with_gemini(text, output_path, speaker, speed, style_prompt, model_name=target_model)
        
    else:
        # Clova (Default)
        success = _generate_with_clova(text, output_path, clova_speaker_id, speed, pitch, volume, emotion, emotion_strength)
        
        # Clova ì‹¤íŒ¨ ì‹œ Edge TTS í´ë°±
        if not success and use_edge_fallback:
            print(f"    Edge TTS í´ë°± ì‹œë„...")
            success = edge_tts_fallback(text, output_path, get_edge_voice_type(speaker))

    # 4. ê²°ê³¼ ìºì‹±
    if success and use_cache:
        tts_core._TTS_CACHE.set(cache_key, output_path)

    return success



def generate_audio_with_character_voices(
    subtitle: str,
    output_dir: Path,
    uid: str,
    scene_idx: int,
    known_characters: List[str] = None,
    voice_assignments: Dict[str, str] = None
) -> Path:
    """
    ìë§‰ì„ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„ë¦¬í•˜ì—¬ ìºë¦­í„°ë³„ ë‹¤ë¥¸ ëª©ì†Œë¦¬ë¡œ TTS ìƒì„±

    Args:
        subtitle: ìë§‰ í…ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ í´ë”
        uid: ê³ ìœ  ID
        scene_idx: ì¥ë©´ ì¸ë±ìŠ¤
        known_characters: ë“±ì¥ì¸ë¬¼ ëª©ë¡
        voice_assignments: ìºë¦­í„°ë³„ ìŒì„± ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ (ê³ ì •ìš©)

    Returns:
        ìƒì„±ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ë˜ëŠ” None)
    """
    if not subtitle or not subtitle.strip():
        return None

    known_characters = known_characters or []
    voice_assignments = voice_assignments or {}

    # ìë§‰ì„ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„ë¦¬
    segments = tts_core.parse_dialogue_with_speaker(subtitle, known_characters)

    print(f"   ì¥ë©´ {scene_idx+1}: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„ë¦¬")

    def get_voice_with_assignments(speaker: str) -> str:
        """voice_assignments ìš°ì„ , ì—†ìœ¼ë©´ ìë™ ì¶”ë¡ """
        normalized = tts_core.normalize_character(speaker)
        # 1. voice_assignmentsì—ì„œ ì°¾ê¸°
        if speaker in voice_assignments:
            return voice_assignments[speaker]
        if normalized in voice_assignments:
            return voice_assignments[normalized]
        # 2. ìë™ ì¶”ë¡ 
        return tts_core.get_voice_for_character(speaker)

    # ì„¸ê·¸ë¨¼íŠ¸ê°€ 1ê°œë©´ ë‹¨ì¼ TTS
    if len(segments) == 1:
        seg = segments[0]
        voice_alias = get_voice_with_assignments(seg["speaker"])

        output_path = output_dir / f"tts_{scene_idx:02d}_{uid}.mp3"

        if text_to_speech(seg["text"], str(output_path), speaker=voice_alias):
            print(f"     {seg['speaker']} â†’ {voice_alias}")
            return output_path
        return None

    # ì—¬ëŸ¬ ì„¸ê·¸ë¨¼íŠ¸ â†’ ê°ê° TTS í›„ í•©ì„±
    temp_paths = []
    for i, seg in enumerate(segments):
        voice_alias = get_voice_with_assignments(seg["speaker"])

        temp_path = output_dir / f"tts_{scene_idx:02d}_{uid}_seg{i:02d}.mp3"

        if text_to_speech(seg["text"], str(temp_path), speaker=voice_alias):
            temp_paths.append(str(temp_path))
            print(f"    [{seg['type']}] {seg['speaker']} â†’ {voice_alias}")
        else:
            print(f"    [{seg['type']}] {seg['speaker']} TTS ì‹¤íŒ¨")

    if not temp_paths:
        return None

    # í•˜ë‚˜ì˜ íŒŒì¼ë¡œ í•©ì„±
    output_path = output_dir / f"tts_{scene_idx:02d}_{uid}.mp3"

    if len(temp_paths) == 1:
        # í•˜ë‚˜ë§Œ ì„±ê³µí•˜ë©´ ê·¸ëƒ¥ ì´ë™
        shutil.move(temp_paths[0], str(output_path))
    else:
        # ì—¬ëŸ¬ ê°œ í•©ì„±
        if tts_core.concat_audio_files(temp_paths, str(output_path)):
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for tp in temp_paths:
                try:
                    Path(tp).unlink()
                except OSError:
                    pass  # íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ë¬´ì‹œ
        else:
            # í•©ì„± ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ íŒŒì¼ë§Œ ì‚¬ìš©
            shutil.move(temp_paths[0], str(output_path))

    return output_path if output_path.exists() else None


def generate_audio_for_subtitles_with_characters(
    subtitles: list,
    output_dir: Path,
    uid: str,
    known_characters: List[str] = None,
    voice_assignments: Dict[str, str] = None
) -> list:
    """
    ìë§‰ ë¦¬ìŠ¤íŠ¸ë¥¼ ìºë¦­í„°ë³„ ëª©ì†Œë¦¬ë¡œ ìŒì„± ë³€í™˜ (ê°™ì€ ìºë¦­í„° = ê°™ì€ ëª©ì†Œë¦¬ ë³´ì¥)

    Args:
        subtitles: ìë§‰ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ í´ë”
        uid: ê³ ìœ  ID
        known_characters: ë“±ì¥ì¸ë¬¼ ëª©ë¡
        voice_assignments: ìºë¦­í„°ë³„ ìŒì„± ê³ ì • ë§¤í•‘ (ì„¸ì…˜ ë ˆë²¨)

    Returns:
        ìƒì„±ëœ ìŒì„± íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    audio_paths = []
    known_characters = known_characters or []
    voice_assignments = voice_assignments or {}

    # voice_assignments ë¡œê·¸ ì¶œë ¥
    if voice_assignments:
        print(f"  ğŸ¤ ê³ ì •ëœ ìŒì„± ë§¤í•‘: {len(voice_assignments)}ê°œ ìºë¦­í„°")

    for i, text in enumerate(subtitles):
        if not text or not text.strip():
            audio_paths.append(None)
            print(f"  ğŸ–¼ ì¥ë©´ {i+1} ìë§‰ ì—†ìŒ (ìŠ¤í‚µ)")
            continue

        audio_path = generate_audio_with_character_voices(
            text, output_dir, uid, i, known_characters,
            voice_assignments=voice_assignments  # ê³ ì •ëœ ë§¤í•‘ ì „ë‹¬!
        )

        if audio_path:
            audio_paths.append(audio_path)
            print(f"   ì¥ë©´ {i+1} ìŒì„± ìƒì„± ì™„ë£Œ")
        else:
            audio_paths.append(None)
            print(f"   ì¥ë©´ {i+1} ìŒì„± ìƒì„± ì‹¤íŒ¨")

    return audio_paths



# ==========================================
# 3. ë©”ì¸ ì¸í„°í˜ì´ìŠ¤ (Wrapper)
# ==========================================

# ============================================================
# ìŠ¤í”¼ì»¤ ë¬¸ìì—´ íŒŒì‹± ë° í™”ì ë°°ì • ë¡œì§ ê°œì„ 
# ============================================================

def parse_speaker_info(speaker_str: str):
    """
    Step 3ì˜ ë³µí•© ìŠ¤í”¼ì»¤ ë¬¸ìì—´ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    ì…ë ¥ ì˜ˆ: "char_01 (í¥ë¶€) adult_male" ë˜ëŠ” "narrator"
    ë°˜í™˜: (voice_type, character_name)
    """
    if not speaker_str:
        return "narrator", None
    
    # 1. ê´„í˜¸ê°€ ìˆëŠ” í¬ë§·: "ID (ì´ë¦„) íƒ€ì…"
    # ì˜ˆ: char_01 (í¥ë¶€) adult_male -> ì´ë¦„: í¥ë¶€, íƒ€ì…: adult_male
    match = re.search(r'\((.*?)\)\s*([a-zA-Z0-9_]+)', speaker_str)
    if match:
        char_name = match.group(1) # í¥ë¶€
        voice_type = match.group(2) # adult_male
        return voice_type, char_name
    
    # 2. ê´„í˜¸ëŠ” ì—†ì§€ë§Œ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ìš° (ì˜ˆë¹„)
    parts = speaker_str.split()
    if len(parts) >= 2:
        # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ë³´í†µ voice_type
        voice_type = parts[-1]
        char_name = "_".join(parts[:-1]) # ë‚˜ë¨¸ì§€ë¥¼ ì´ë¦„ìœ¼ë¡œ ê°„ì£¼
        return voice_type, char_name

    # 3. ë‹¨ì¼ ë¬¸ìì—´ (ì˜ˆ: "narrator", "child_male")
    return speaker_str, None

def text_to_speech(
    text: str,
    output_path: str,
    speaker: str = "narrator",
    speed: int = 0,
    pitch: int = 0,
    volume: int = 0,
    use_edge_fallback: bool = True,
    character_name: str = None,
    session_id: str = None,
    engine: str = "clova",      #  ì—”ì§„ ì„ íƒ (ê¸°ë³¸ê°’ clova)
    style_prompt: str = ""      # gpt, gemini ìš© í”„ë¡¬í”„íŠ¸
) -> bool:
    """
    í…ìŠ¤íŠ¸ë¥¼ í´ë¡œë°” TTSë¡œ ìŒì„± ë³€í™˜ (Edge TTS í´ë°± ì§€ì›)

    Args:
        text: ë³€í™˜í•  í…ìŠ¤íŠ¸ (2000ì ì´ˆê³¼ ì‹œ ìë™ ë¶„í• )
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (.mp3)
        speaker: í™”ì í‚¤ (narrator, child_male ë“±) ë˜ëŠ” Clova ID
        speed: ì†ë„ (-5 ~ 5, ê¸°ë³¸ 0)
        pitch: í”¼ì¹˜ (-5 ~ 5, ê¸°ë³¸ 0)
        volume: ë³¼ë¥¨ (-5 ~ 5, ê¸°ë³¸ 0)
        use_edge_fallback: Clova ì‹¤íŒ¨ ì‹œ Edge TTS ì‚¬ìš© ì—¬ë¶€
        character_name: ìºë¦­í„° ì´ë¦„ (ì„¸ì…˜ ë‚´ ìŒì„± ì¼ê´€ì„± ìœ ì§€ìš©)
        session_id: ì„¸ì…˜ ID (ì±…/í”„ë¡œì íŠ¸ ë‹¨ìœ„)

    Returns:
        ì„±ê³µ ì—¬ë¶€ (True/False)
    """
    if not text or not text.strip():
        return False

    # 1. í…ìŠ¤íŠ¸ ì •ê·œí™” (ë”°ì˜´í‘œ, ê³µë°± ì •ë¦¬)
    text = tts_core.normalize_text(text)
    if not text:
        return False

    # Clovaì¼ ë•Œë§Œ SessionManager ì‚¬ìš©
    if engine == "clova":
        if speaker in tts_core.VOICE_POOLS or speaker in tts_core.VOICE_ALIASES:
            session_mgr = tts_core.get_session_voice_manager(session_id)
            speaker = session_mgr.get_clova_voice_id(speaker, character_name)

    # ì—”ì§„ë³„ ì œí•œì— ë§ì¶° í…ìŠ¤íŠ¸ ë¶„í• 
    limit = LIMITS.get(engine, 2000)
    text_chunks = tts_core.split_text_safely(text, limit=limit)

    if len(text_chunks) == 1:
        return _text_to_speech_single(
            text_chunks[0], output_path, speaker,
            speed, pitch, volume, use_edge_fallback=use_edge_fallback,
            engine=engine, style_prompt=style_prompt
        )

    # ë‹¤ì¤‘ ì²­í¬ ì²˜ë¦¬
    temp_files = []
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)

    for i, chunk in enumerate(text_chunks):
        temp_path = str(output_dir / f"_chunk_{i:02d}.mp3")
        if _text_to_speech_single(
            chunk, temp_path, speaker, speed, pitch, volume,
            use_edge_fallback=use_edge_fallback,
            engine=engine, style_prompt=style_prompt
        ):
            temp_files.append(temp_path)
        else:
            print(f"   ì²­í¬ {i+1} ì‹¤íŒ¨")

    if temp_files:
        try:
            tts_core.concat_audio_files(temp_files, output_path)
            for tf in temp_files: Path(tf).unlink(missing_ok=True)
            return True
        except: return False

    return False


# ==========================================
# ==========================================



def generate_audio_for_subtitles(
    subtitles: list,
    output_dir: Path,
    uid: str,
    speaker: str = "narrator",
    speakers: List[str] = None,
    parallel: bool = True,
    max_workers: int = 5,
    split_narration: bool = True,
    engine: str = "clova",
    global_speed: int = 0,
    style_prompts: List[str] = None
) -> list:
    """
    ìë§‰ ë¦¬ìŠ¤íŠ¸ë¥¼ ìŒì„± íŒŒì¼ë“¤ë¡œ ë³€í™˜ (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)

    Args:
        subtitles: ìë§‰ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ í´ë”
        uid: ê³ ìœ  ID
        speaker: ë‹¨ì¼ í™”ì í‚¤ (ëª¨ë“  ìë§‰ì— ë™ì¼ ì ìš©)
        speakers: ìë§‰ë³„ í™”ì ë¦¬ìŠ¤íŠ¸ (GPTê°€ ë°°ì •í•œ í™”ì) - speakerë³´ë‹¤ ìš°ì„ 
        parallel: ë³‘ë ¬ ì²˜ë¦¬ ì—¬ë¶€ (ê¸°ë³¸ True)
        max_workers: ìµœëŒ€ ë™ì‹œ ì‘ì—… ìˆ˜ (API rate limit ê³ ë ¤)
        split_narration: ë‚˜ë˜ì´ì…˜/ëŒ€ì‚¬ ë¶„ë¦¬ ì—¬ë¶€ (ê¸°ë³¸ True)
            - True: ë‚˜ë˜ì´ì…˜ì€ narrator, ëŒ€ì‚¬ë§Œ GPT í™”ì ì ìš©
            - False: ê¸°ì¡´ ë™ì‘ (ì „ì²´ì— GPT í™”ì ì ìš©)

    Returns:
        ìƒì„±ëœ ìŒì„± íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed

    audio_paths = [None] * len(subtitles)
    # Gemini ì—”ì§„ ê°ì§€ ë° ìŠ¤ë¡œí‹€ë§(Throttling) ì„¤ì •
    is_gemini = "gemini" in engine.lower()
    
    # 1. Gemini ì—”ì§„ ì—¬ë¶€ í™•ì¸
    is_gemini = "gemini" in engine.lower()

    # 2. ì‘ì—…ì(Worker) ìˆ˜ ë° ë³‘ë ¬ ì„¤ì • ì¡°ì •
    if is_gemini:
        # [Gemini] 429 ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ë™ì‹œ ì‹¤í–‰ ìˆ˜ë¥¼ 3ê°œë¡œ ì œí•œ
        # (ê¸°ì¡´ì— 5ê°œì˜€ìœ¼ë©´ 3ê°œë¡œ ì¤„ì„, parallelì€ ìœ ì§€í•˜ë˜ ì†ë„ ì¡°ì ˆ)
        run_max_workers = 3 
        print(f"â„¹ï¸ [Gemini] ì•ˆì •ì„±ì„ ìœ„í•´ ë³‘ë ¬ ì‘ì—… ìˆ˜ë¥¼ {run_max_workers}ê°œë¡œ ì œí•œí•©ë‹ˆë‹¤.")
    else:
        # [Clova/GPT] ê¸°ì¡´ ì„¤ì • ê·¸ëŒ€ë¡œ (ë³´í†µ 5ê°œ) -> ë”œë ˆì´ ì—†ì´ ë¹ ë¥´ê²Œ ì²˜ë¦¬
        run_max_workers = max_workers

    # speakers ë¦¬ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë‹¨ì¼ speakerë¡œ ì±„ì›€
    if speakers is None:
        speakers = [speaker] * len(subtitles)
    elif len(speakers) < len(subtitles):
        # ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€
        speakers = speakers + [speaker] * (len(subtitles) - len(speakers))

    #  ì „ì²´ ìŠ¤í”¼ì»¤ ë¦¬ìŠ¤íŠ¸ì—ì„œ 'ë“±ì¥ì¸ë¬¼ ì´ë¦„'ì„ ë¯¸ë¦¬ ì¶”ì¶œí•˜ì—¬ known_characters êµ¬ì¶•
    # ì´ë¥¼ í†µí•´ ë‚˜ë ˆì´ì…˜ ì† ëŒ€í™”ë¬¸ì˜ ì£¼ì¸ì„ ë” ì˜ ì°¾ê²Œ ë§Œë“­ë‹ˆë‹¤.
    collected_characters = set()
    for s in speakers:
        _, name = parse_speaker_info(s)
        if name:
            collected_characters.add(name)
    known_char_list = list(collected_characters)

    # GPT speaker type â†’ VOICE_ALIASES key ë§¤í•‘ (í™•ì¥)
    SPEAKER_TYPE_MAP = {
        "narrator": "narrator",
        "child": "child_male",  # ê¸°ë³¸ê°’ (GPTê°€ ì„±ë³„ ë¯¸ì§€ì • ì‹œ)
        "child_male": "child_male",
        "child_female": "child_female",
        "elder_female": "elder_female",
        "elder_male": "elder_male",
        "adult_female": "adult_female",
        "adult_male": "adult_male",
        "young_female": "young_female",
        "young_male": "young_male",
        "animal": "cute_animal",
        "fairy": "fairy",
        "demon": "elder_female",
        "none": None,  # ìŒì„± ìƒì„± ì•ˆ í•¨
    }
    # style_promptsê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™” (ì—ëŸ¬ ë°©ì§€)
    if style_prompts is None:
        style_prompts = [""] * len(subtitles)
        

    def process_subtitle_with_split(i: int, text: str, raw_spk_str: str, specific_prompt: str):
        """
        ë‚˜ë˜ì´ì…˜/ëŒ€ì‚¬ ë¶„ë¦¬ + ê°œì„ ëœ í™”ì ë§¤í•‘ ì ìš©
        (GPT/Gemini ì‚¬ìš© ì‹œ Edge TTSë¡œ ë¹ ì§€ëŠ” ê²ƒ ë°©ì§€)
        """
        # Geminiì¼ ê²½ìš° ìš”ì²­ ì‹œì‘ ì „ ì ì‹œ ëŒ€ê¸° (Rate Limiting)
        if is_gemini:
            time.sleep(0.2)  
        if not text or not text.strip():
            return i, None, "skip", raw_spk_str

        # =========================================================================
        # [1] í™”ì ì •ë³´ íŒŒì‹± ë¡œì§
        # =========================================================================
        
        final_voice_key = "narrator"  # ê¸°ë³¸ê°’ ì´ˆê¸°í™”
        main_char_name = None
        found_valid_type = False

        # 1. ê´„í˜¸ ì•ˆì— ìˆëŠ” ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: 'ë§ˆë…€ í• ë¨¸ë‹ˆ')
        name_match = re.search(r'\((.*?)\)', raw_spk_str)
        if name_match:
            main_char_name = name_match.group(1).strip()
        
        # 2. ë³´ì´ìŠ¤ íƒ€ì… ì¶”ì¶œ (ë’¤ì—ì„œë¶€í„° ê²€ìƒ‰)
        parts = raw_spk_str.split()
        for part in reversed(parts):
            clean_part = part.strip("(),")
            if clean_part in SPEAKER_TYPE_MAP:
                final_voice_key = SPEAKER_TYPE_MAP[clean_part]
                found_valid_type = True
                break
        
        # 3. 1ì°¨ ì‹œë„ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ íŒŒì‹± í•¨ìˆ˜ ì‹œë„
        if not found_valid_type:
            p_type, p_name = parse_speaker_info(raw_spk_str)
            if p_type in SPEAKER_TYPE_MAP:
                final_voice_key = SPEAKER_TYPE_MAP[p_type]
                found_valid_type = True
            
            if not main_char_name and p_name:
                main_char_name = p_name

        # 4. ì´ë¦„ ê°•ì œ ì„¤ì • (Priority 1 ë°œë™ìš©)
        if not main_char_name and found_valid_type and final_voice_key != "narrator":
            main_char_name = raw_spk_str

        # =========================================================================
        # [2]  ì—”ì§„ë³„ Strict Mode ì ìš© 
        # =========================================================================
        
        # í˜„ì¬ ì—”ì§„ì´ í”„ë¦¬ë¯¸ì—„ ì—”ì§„(GPT, Gemini)ì¸ì§€ í™•ì¸
        is_premium_engine = any(k in engine.lower() for k in ["gpt", "gemini"])

        if is_premium_engine:
            # GPTë‚˜ Geminiì¸ë° ìœ íš¨í•œ í™”ì íƒ€ì…(child_male ë“±)ì„ ëª» ì°¾ì•˜ë‹¤ë©´?
            # ì ˆëŒ€ Edge TTSë¡œ ë„˜ê¸°ì§€ ë§ê³ , ë¬´ì¡°ê±´ 'narrator'(ê¸°ë³¸ GPT ëª©ì†Œë¦¬)ë¡œ ê³ ì •!
            if not found_valid_type:
                final_voice_key = "narrator"
        
        # (ì°¸ê³ : ClovaëŠ” ìœ„ ì¡°ê±´ì— ê±¸ë¦¬ì§€ ì•Šìœ¼ë¯€ë¡œ, ë§¤í•‘ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ì²˜ëŸ¼ ë„˜ì–´ê°€ì„œ
        #  ë‚´ë¶€ ë¡œì§ì— ë”°ë¼ Edge TTSë‚˜ ë‹¤ë¥¸ ëŒ€ì•ˆì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë‘ )

        # 5. ìµœì¢… í‚¤ í• ë‹¹
        scene_voice_key = final_voice_key

        if scene_voice_key is None:
            return i, None, "skip", raw_spk_str

        # =========================================================================
        # ì™¸ë¶€ì—ì„œ ë„˜ì–´ì˜¨ specific_promptê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ìµœìš°ì„ ìœ¼ë¡œ ì‚¬ìš©
        if specific_prompt:
            style_prompt = specific_prompt
        else:
            # ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            style_prompt = ""
            # [ì‹ ê·œ] Geminiìš© í”„ë¡¬í”„íŠ¸ ë¡œì§ (í•„ìš”ì‹œ í™•ì¥ ê°€ëŠ¥)
            if "gemini" in engine.lower():
                if "child" in scene_voice_key: style_prompt = "Speak like a young child."
                elif "anger" in scene_voice_key: style_prompt = "Speak with an angry tone."
            # GPTìš© í”„ë¡¬í”„íŠ¸
            elif "gpt" in engine.lower():
                if "child" in scene_voice_key: style_prompt = "Speak like a cute child."
                elif "anger" in scene_voice_key: style_prompt = "Speak angrily."
            
        # í…ìŠ¤íŠ¸ ë¶„ë¦¬
        segments = tts_core.parse_dialogue_with_speaker(text, known_characters=known_char_list)

        # ----------------------------------------------------------------
        # [Helper] ì„¸ê·¸ë¨¼íŠ¸ë³„ í™”ì ê²°ì • ë¡œì§
        # ----------------------------------------------------------------
        def resolve_voice(segment):
            seg_type = segment["type"]
            inferred_speaker = segment.get("speaker") 

            # A. ë‚˜ë ˆì´ì…˜
            if seg_type == "narration":
                if len(segments) == 1 and scene_voice_key != "narrator":
                    return scene_voice_key, main_char_name
                return "narrator", None

            # B. ëŒ€í™”ë¬¸
            # (Priority 1) ì‚¬ìš©ì UI ì„¤ì • (ì´ë¦„ì´ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì´ê±¸ë¡œ)
            if main_char_name and scene_voice_key:
                return scene_voice_key, main_char_name
            
            # (Priority 2) ë¬¸ë§¥ ì¶”ë¡ 
            if inferred_speaker and inferred_speaker != "narrator":
                inferred_voice_type = tts_core.get_voice_for_character(inferred_speaker)
                # â­ï¸ ì¤‘ìš”: í”„ë¦¬ë¯¸ì—„ ì—”ì§„ì¼ ë•Œ ì¶”ë¡  ê²°ê³¼ê°€ ì—‰ëš±í•˜ë©´ narratorë¡œ ë°©ì–´
                if is_premium_engine and inferred_voice_type not in SPEAKER_TYPE_MAP.values():
                    return "narrator", inferred_speaker
                
                return inferred_voice_type, inferred_speaker
            
            # (Priority 3) ê¸°ë³¸ê°’ (ì´ë¯¸ ìœ„ì—ì„œ ë°©ì–´ë¨)
            return scene_voice_key, None
        # ----------------------------------------------------------------

        # (A) ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
        if len(segments) == 1:
            seg = segments[0]
            voice_key, char_name = resolve_voice(seg)
            
            audio_path = output_dir / f"tts_{i:02d}_{uid}.mp3"
            success = text_to_speech(
                seg["text"], str(audio_path), speaker=voice_key, speed=global_speed,
                character_name=char_name, session_id=uid, engine=engine, style_prompt=style_prompt
            )
            return i, audio_path if success else None, "ok" if success else "fail", f"{raw_spk_str}"

        # (B) ë‹¤ì¤‘ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
        temp_paths = []
        segment_info = []

        for seg_idx, seg in enumerate(segments):
            #  ì„¸ê·¸ë¨¼íŠ¸ ì‚¬ì´ì—ë„ ë”œë ˆì´ í•„ìš” (í•œ ë¬¸ì¥ì´ 3ê°œë¡œ ìª¼ê°œì§ˆ ë•Œ í­ì£¼ ë°©ì§€)
            if is_gemini and seg_idx > 0:
                time.sleep(0.1)
            voice_key, char_name = resolve_voice(seg)
            temp_path = output_dir / f"tts_{i:02d}_{uid}_seg{seg_idx:02d}.mp3"

            if text_to_speech(
                seg["text"], str(temp_path), speaker=voice_key, speed=global_speed,
                character_name=char_name, session_id=uid, engine=engine, style_prompt=style_prompt
            ):
                temp_paths.append(str(temp_path))
                segment_info.append(f"{seg['type']}")
            else:
                print(f"    âš ï¸ [Segment Fail] Scene {i}-{seg_idx}: {seg['text'][:10]}...")

        if not temp_paths:
            return i, None, "fail", raw_spk_str

        audio_path = output_dir / f"tts_{i:02d}_{uid}.mp3"

        if len(temp_paths) == 1:
            shutil.move(temp_paths[0], str(audio_path))
        else:
            if tts_core.concat_audio_files(temp_paths, str(audio_path)):
                for tp in temp_paths:
                    try: Path(tp).unlink()
                    except: pass
            else:
                shutil.move(temp_paths[0], str(audio_path))

        return i, audio_path if audio_path.exists() else None, "ok", "+".join(segment_info)

    # ë¶„ë¦¬ ëª¨ë“œ ì„ íƒ
    process_func = process_subtitle_with_split

    if parallel and len(subtitles) > 1:
        # ë³‘ë ¬ ì²˜ë¦¬ (5-10ë°° ì†ë„ í–¥ìƒ)
        with ThreadPoolExecutor(max_workers=run_max_workers) as executor:
            futures = {
                executor.submit(process_subtitle_with_split, i, subtitles[i], speakers[i], style_prompts[i]): i
                for i in range(len(subtitles))
            }

            for future in as_completed(futures):
                idx, result, status, spk_info = future.result()
                audio_paths[idx] = result
                if status == "skip":
                    print(f"  [SKIP] Scene {idx+1} - no subtitle")
                elif status == "ok":
                    print(f"  [OK] Scene {idx+1} [{spk_info}] audio generated")
                else:
                    print(f"  [FAIL] Scene {idx+1} [{spk_info}] audio failed")
    else:
        # ìˆœì°¨ ì²˜ë¦¬ (ë‹¨ì¼ ìë§‰ ë˜ëŠ” parallel=False)
        for i, text in enumerate(subtitles):
            idx, result, status, spk_info = process_subtitle_with_split(i, subtitles[i], speakers[i], style_prompts[i])
            audio_paths[idx] = result
            if status == "skip":
                print(f"  [SKIP] Scene {idx+1} - no subtitle")
            elif status == "ok":
                print(f"  [OK] Scene {idx+1} [{spk_info}] audio generated")
            else:
                print(f"  [FAIL] Scene {idx+1} [{spk_info}] audio failed")

    return audio_paths

